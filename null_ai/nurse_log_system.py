"""
NullAI å€’æœ¨ã‚·ã‚¹ãƒ†ãƒ  (NurseLog System)

æ£®æ—ã®å€’æœ¨ãŒæ¬¡ä¸–ä»£ã®æ „é¤Šã¨ãªã‚‹ã‚ˆã†ã«ã€
å¤ã„ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ–°ãƒ¢ãƒ‡ãƒ«ã«ç¶™æ‰¿ã™ã‚‹ä¸–ä»£äº¤ä»£ã‚·ã‚¹ãƒ†ãƒ ã€‚

æ©Ÿèƒ½:
1. æ¨è«–å±¥æ­´ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
2. è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (JSONL, CSV, Parquet, HuggingFace Datasets)
3. æ–°ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
4. DBä¿ç®¡ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æº
"""
import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional, Literal
from datetime import datetime
from pathlib import Path
import hashlib

logger = logging.getLogger(__name__)

ExportFormat = Literal["jsonl", "csv", "parquet", "hf_dataset"]


class InferenceHistory:
    """æ¨è«–å±¥æ­´ã®ç®¡ç†"""

    def __init__(self, storage_path: str = "inference_history"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

    def save_inference(
        self,
        question: str,
        response: str,
        domain_id: str,
        model_name: str,
        confidence: float,
        thinking_process: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ¨è«–çµæœã‚’ä¿å­˜"""
        inference_id = hashlib.sha256(
            f"{question}{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]

        record = {
            "inference_id": inference_id,
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "response": response,
            "domain_id": domain_id,
            "model_name": model_name,
            "confidence": confidence,
            "thinking_process": thinking_process,
            "metadata": metadata or {}
        }

        # æ—¥ä»˜ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²
        date_str = datetime.now().strftime("%Y-%m-%d")
        file_path = self.storage_path / f"history_{date_str}.jsonl"

        with open(file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

        logger.info(f"Saved inference {inference_id} to {file_path}")
        return inference_id

    def load_history(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        domain_id: Optional[str] = None,
        min_confidence: float = 0.0
    ) -> List[Dict[str, Any]]:
        """æ¨è«–å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
        history = []

        for file_path in sorted(self.storage_path.glob("history_*.jsonl")):
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        record = json.loads(line)

                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if domain_id and record.get("domain_id") != domain_id:
                            continue
                        if record.get("confidence", 0) < min_confidence:
                            continue
                        if start_date and record.get("timestamp", "") < start_date:
                            continue
                        if end_date and record.get("timestamp", "") > end_date:
                            continue

                        history.append(record)

        return history


class TrainingDataExporter:
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""

    def __init__(self, output_dir: str = "training_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def export_jsonl(
        self,
        history: List[Dict[str, Any]],
        output_file: str,
        format_type: Literal["chat", "instruction", "completion"] = "chat"
    ) -> str:
        """JSONLå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        output_path = self.output_dir / output_file

        with open(output_path, "w", encoding="utf-8") as f:
            for record in history:
                if format_type == "chat":
                    # ChatMLå½¢å¼
                    item = {
                        "messages": [
                            {"role": "system", "content": f"You are an expert in {record['domain_id']}."},
                            {"role": "user", "content": record["question"]},
                            {"role": "assistant", "content": record["response"]}
                        ]
                    }
                elif format_type == "instruction":
                    # Instructionå½¢å¼
                    item = {
                        "instruction": record["question"],
                        "output": record["response"],
                        "input": "",
                        "domain": record["domain_id"]
                    }
                else:  # completion
                    # Completionå½¢å¼
                    item = {
                        "prompt": record["question"],
                        "completion": record["response"]
                    }

                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        logger.info(f"Exported {len(history)} records to {output_path} ({format_type} format)")
        return str(output_path)

    def export_csv(
        self,
        history: List[Dict[str, Any]],
        output_file: str
    ) -> str:
        """CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        import csv

        output_path = self.output_dir / output_file

        with open(output_path, "w", encoding="utf-8", newline="") as f:
            if not history:
                return str(output_path)

            fieldnames = ["question", "response", "domain_id", "model_name", "confidence", "timestamp"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            for record in history:
                writer.writerow({
                    "question": record["question"],
                    "response": record["response"],
                    "domain_id": record["domain_id"],
                    "model_name": record["model_name"],
                    "confidence": record["confidence"],
                    "timestamp": record["timestamp"]
                })

        logger.info(f"Exported {len(history)} records to {output_path} (CSV format)")
        return str(output_path)

    def export_parquet(
        self,
        history: List[Dict[str, Any]],
        output_file: str
    ) -> str:
        """Parquetå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            import pandas as pd
            import pyarrow.parquet as pq

            output_path = self.output_dir / output_file

            df = pd.DataFrame([
                {
                    "question": r["question"],
                    "response": r["response"],
                    "domain_id": r["domain_id"],
                    "model_name": r["model_name"],
                    "confidence": r["confidence"],
                    "timestamp": r["timestamp"]
                }
                for r in history
            ])

            df.to_parquet(output_path, engine="pyarrow", compression="snappy")

            logger.info(f"Exported {len(history)} records to {output_path} (Parquet format)")
            return str(output_path)

        except ImportError:
            logger.error("pandas and pyarrow are required for Parquet export. Install with: pip install pandas pyarrow")
            raise

    def export_hf_dataset(
        self,
        history: List[Dict[str, Any]],
        dataset_name: str,
        push_to_hub: bool = False,
        hub_repo: Optional[str] = None
    ) -> str:
        """HuggingFace Datasetå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            from datasets import Dataset

            data = {
                "question": [r["question"] for r in history],
                "response": [r["response"] for r in history],
                "domain_id": [r["domain_id"] for r in history],
                "confidence": [r["confidence"] for r in history]
            }

            dataset = Dataset.from_dict(data)

            # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
            output_path = self.output_dir / dataset_name
            dataset.save_to_disk(str(output_path))

            logger.info(f"Exported {len(history)} records to {output_path} (HF Dataset format)")

            # HuggingFace Hubã«ãƒ—ãƒƒã‚·ãƒ¥
            if push_to_hub and hub_repo:
                dataset.push_to_hub(hub_repo)
                logger.info(f"Pushed dataset to HuggingFace Hub: {hub_repo}")

            return str(output_path)

        except ImportError:
            logger.error("datasets library is required for HF Dataset export. Install with: pip install datasets")
            raise


class ModelSuccessionManager:
    """ãƒ¢ãƒ‡ãƒ«ä¸–ä»£äº¤ä»£ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(
        self,
        history_manager: InferenceHistory,
        exporter: TrainingDataExporter,
        succession_threshold: int = 1000
    ):
        self.history_manager = history_manager
        self.exporter = exporter
        self.succession_threshold = succession_threshold
        self.succession_log_path = Path("succession_log.json")

    def check_succession_trigger(self) -> bool:
        """ä¸–ä»£äº¤ä»£ã®ãƒˆãƒªã‚¬ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯"""
        history = self.history_manager.load_history()

        # é«˜å“è³ªãªæ¨è«–ãŒé–¾å€¤ã‚’è¶…ãˆãŸã‚‰ä¸–ä»£äº¤ä»£
        high_quality = [r for r in history if r.get("confidence", 0) >= 0.8]

        return len(high_quality) >= self.succession_threshold

    async def prepare_succession(
        self,
        domain_id: Optional[str] = None,
        min_confidence: float = 0.8
    ) -> Dict[str, Any]:
        """ä¸–ä»£äº¤ä»£ã®æº–å‚™"""
        logger.info(f"Preparing succession for domain: {domain_id or 'all'}")

        # é«˜å“è³ªãªæ¨è«–å±¥æ­´ã‚’å–å¾—
        history = self.history_manager.load_history(
            domain_id=domain_id,
            min_confidence=min_confidence
        )

        if len(history) < 100:
            logger.warning(f"Not enough high-quality inferences: {len(history)}")
            return {"status": "insufficient_data", "count": len(history)}

        # è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exports = {}

        # JSONL (ChatMLå½¢å¼)
        jsonl_file = f"training_data_{timestamp}.jsonl"
        exports["jsonl"] = self.exporter.export_jsonl(
            history, jsonl_file, format_type="chat"
        )

        # CSV
        csv_file = f"training_data_{timestamp}.csv"
        exports["csv"] = self.exporter.export_csv(history, csv_file)

        # Parquetã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
        try:
            parquet_file = f"training_data_{timestamp}.parquet"
            exports["parquet"] = self.exporter.export_parquet(history, parquet_file)
        except ImportError:
            logger.info("Skipping Parquet export (pandas not installed)")

        try:
            dataset_name = f"dataset_{timestamp}"
            exports["hf_dataset"] = self.exporter.export_hf_dataset(
                history, dataset_name
            )
        except ImportError:
            logger.info("Skipping HF Dataset export (datasets not installed)")

        # ä¸–ä»£äº¤ä»£ãƒ­ã‚°ã‚’ä¿å­˜
        succession_log = {
            "timestamp": datetime.now().isoformat(),
            "domain_id": domain_id,
            "records_count": len(history),
            "min_confidence": min_confidence,
            "exports": exports
        }

        self._append_succession_log(succession_log)

        logger.info(f"Succession preparation complete: {len(history)} records exported")

        return {
            "status": "prepared",
            "count": len(history),
            "exports": exports,
            "log": succession_log
        }

    def _append_succession_log(self, log_entry: Dict[str, Any]):
        """ä¸–ä»£äº¤ä»£ãƒ­ã‚°ã‚’è¿½è¨˜"""
        logs = []
        if self.succession_log_path.exists():
            with open(self.succession_log_path, "r", encoding="utf-8") as f:
                logs = json.load(f)

        logs.append(log_entry)

        with open(self.succession_log_path, "w", encoding="utf-8") as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)

    def get_succession_history(self) -> List[Dict[str, Any]]:
        """ä¸–ä»£äº¤ä»£å±¥æ­´ã‚’å–å¾—"""
        if not self.succession_log_path.exists():
            return []

        with open(self.succession_log_path, "r", encoding="utf-8") as f:
            return json.load(f)

    async def create_standalone_package(
        self,
        domain_id: Optional[str] = None,
        min_confidence: float = 0.8,
        db_path: Optional[str] = None,
        output_dir: str = "succession_packages"
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ãªã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½œæˆ

        å«ã¾ã‚Œã‚‹ã‚‚ã®:
        - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ (å…¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
        - Knowledge Base (DB)
        - README.md (ä½¿ã„æ–¹ã‚¬ã‚¤ãƒ‰)
        - run_inference.py (æ¨è«–å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ)
        - config.json (è¨­å®š)

        ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã€å˜ä½“ã§å‹•ä½œã™ã‚‹å®Œå…¨ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã«ãªã‚Šã¾ã™ã€‚
        """
        import zipfile
        import shutil

        logger.info("Creating standalone package with training data and DB")

        # ä¸–ä»£äº¤ä»£ã‚’å®Ÿè¡Œï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰
        succession_result = await self.prepare_succession(
            domain_id=domain_id,
            min_confidence=min_confidence
        )

        if succession_result.get("status") != "prepared":
            return succession_result

        # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        package_name = f"null_ai_model_gen{len(self.get_succession_history())}_{timestamp}"
        package_dir = output_path / package_name
        package_dir.mkdir(exist_ok=True)

        # 1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
        training_dir = package_dir / "training_data"
        training_dir.mkdir(exist_ok=True)

        for format_type, file_path in succession_result["exports"].items():
            if os.path.exists(file_path):
                dest = training_dir / os.path.basename(file_path)
                shutil.copy2(file_path, dest)
                logger.info(f"Copied training data: {file_path} -> {dest}")

        # 2. Knowledge Base (DB) ã‚’ã‚³ãƒ”ãƒ¼
        db_dir = package_dir / "knowledge_base"
        db_dir.mkdir(exist_ok=True)

        if db_path and os.path.exists(db_path):
            db_dest = db_dir / "knowledge.iath"
            shutil.copy2(db_path, db_dest)
            logger.info(f"Copied DB: {db_path} -> {db_dest}")

            # DBçµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ
            try:
                from iath_decoder import IATHDecoder
                decoder = IATHDecoder(str(db_dest))
                tiles = decoder.read_all_tiles()

                db_stats = {
                    "total_tiles": len(tiles),
                    "domains": list(set(t.get("domain_id", "unknown") for t in tiles)),
                    "export_date": datetime.now().isoformat()
                }

                with open(db_dir / "db_stats.json", "w", encoding="utf-8") as f:
                    json.dump(db_stats, f, indent=2, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"Failed to generate DB stats: {e}")

        # 3. README.md ã‚’ç”Ÿæˆ
        readme_content = f"""# NullAI Model Package - Generation {len(self.get_succession_history())}

## ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ¦‚è¦

ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã€NullAIå€’æœ¨ã‚·ã‚¹ãƒ†ãƒ ï¼ˆNurseLog Systemï¼‰ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸ
å®Œå…¨ãªã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ä¸–ä»£: {len(self.get_succession_history())}
{"å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: " + domain_id if domain_id else "å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³"}
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ•°: {succession_result['count']}

## ğŸŒ² å€’æœ¨ã‚·ã‚¹ãƒ†ãƒ ã¨ã¯

æ£®æ—ã®å€’æœ¨ãŒæ¬¡ä¸–ä»£ã®æ „é¤Šæºã¨ãªã‚‹ã‚ˆã†ã«ã€å¤ã„ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ–°ãƒ¢ãƒ‡ãƒ«ã«ç¶™æ‰¿ã™ã‚‹
ä¸–ä»£äº¤ä»£ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«ã¯ã€è“„ç©ã•ã‚ŒãŸé«˜å“è³ªãªæ¨è«–ãƒ‡ãƒ¼ã‚¿ã¨
Knowledge BaseãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ“ å«ã¾ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«

### training_data/
- `*.jsonl`: ChatML/Instruction/Completionå½¢å¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
- `*.csv`: CSVå½¢å¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
- `*.parquet`: Parquetå½¢å¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `dataset_*/`: HuggingFace Datasetså½¢å¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

### knowledge_base/
- `knowledge.iath`: IATHå½¢å¼ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
- `db_stats.json`: DBçµ±è¨ˆæƒ…å ±

### ãã®ä»–
- `README.md`: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
- `run_inference.py`: æ¨è«–å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `config.json`: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

## ğŸš€ ä½¿ã„æ–¹

### 1. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```bash
# HuggingFace Transformersã‚’ä½¿ç”¨
from datasets import load_from_disk
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
dataset = load_from_disk("training_data/dataset_*")

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model = AutoModelForCausalLM.from_pretrained("your-base-model")
tokenizer = AutoTokenizer.from_pretrained("your-base-model")

# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="./output"),
    train_dataset=dataset
)
trainer.train()
```

### 2. æ¨è«–å®Ÿè¡Œ

```bash
# ä»˜å±ã®Knowledge Baseã‚’ä½¿ç”¨ã—ã¦æ¨è«–
python run_inference.py --question "ã‚ãªãŸã®è³ªå•" --db knowledge_base/knowledge.iath

# ã¾ãŸã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
python run_inference.py --model ./output/checkpoint-1000 --question "è³ªå•"
```

### 3. Knowledge Baseã®ç·¨é›†

```bash
# DBã‚’ç›´æ¥ç·¨é›†ï¼ˆè‡ªç”±ã«ç·¨é›†å¯èƒ½ï¼‰
python -c "from iath_decoder import IATHDecoder; decoder = IATHDecoder('knowledge_base/knowledge.iath'); print(decoder.read_all_tiles())"

# ã¾ãŸã¯ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰ç·¨é›†
cp knowledge_base/knowledge.iath knowledge_base/knowledge.iath.backup
# ç·¨é›†...
```

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯å®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ã§ã™ã€‚è‡ªç”±ã«ï¼š
- âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ãƒ»å‰Šé™¤
- âœ… Knowledge Baseã®ç·¨é›†
- âœ… ç‹¬è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¿½åŠ 
- âœ… ãƒ•ã‚©ãƒ¼ã‚¯ã—ã¦ç‹¬è‡ªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆ
- âœ… ä»–ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®ä½¿ç”¨

## ğŸ“Š çµ±è¨ˆæƒ…å ±

- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: {succession_result['count']}
- æœ€å°ä¿¡é ¼åº¦: {min_confidence}
- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼: {', '.join(succession_result['exports'].keys())}

## ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã€NullAIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€éƒ¨ã¨ã—ã¦æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚
è‡ªç”±ã«ä½¿ç”¨ã€ç·¨é›†ã€å†é…å¸ƒã§ãã¾ã™ã€‚

## ğŸŒ ã‚µãƒãƒ¼ãƒˆ

- GitHub: https://github.com/your-org/null-ai
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://null-ai-docs.example.com
- å•ã„åˆã‚ã›: support@null-ai.example.com

---

**Generated by NullAI NurseLog System v1.0.0**
"""

        with open(package_dir / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)

        # 4. run_inference.py ã‚’ç”Ÿæˆ
        inference_script = """#!/usr/bin/env python3
\"\"\"
NullAI Standalone Inference Runner

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«å«ã¾ã‚Œã‚‹Knowledge Baseã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
\"\"\"
import argparse
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def run_inference(question: str, db_path: str = "knowledge_base/knowledge.iath", model_path: str = None):
    \"\"\"æ¨è«–ã‚’å®Ÿè¡Œ\"\"\"
    print(f"Question: {question}")
    print(f"Using DB: {db_path}")

    if model_path:
        print(f"Using Model: {model_path}")
        # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        # TODO: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨æ¨è«–ã®å®Ÿè£…
    else:
        # Knowledge Baseã‹ã‚‰æ¨è«–
        from iath_decoder import IATHDecoder
        decoder = IATHDecoder(db_path)
        tiles = decoder.read_all_tiles()

        # ç°¡å˜ãªé¡ä¼¼åº¦æ¤œç´¢
        relevant_tiles = []
        for tile in tiles:
            content = tile.get("content", "")
            if any(word.lower() in content.lower() for word in question.split()):
                relevant_tiles.append(tile)

        if relevant_tiles:
            print("\\nRelevant Knowledge:")
            for tile in relevant_tiles[:3]:
                print(f"- {tile.get('topic', 'Unknown')}: {tile.get('content', '')[:200]}...")
        else:
            print("\\nNo relevant knowledge found in the database.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="NullAI Inference Runner")
    parser.add_argument("--question", type=str, required=True, help="Question to answer")
    parser.add_argument("--db", type=str, default="knowledge_base/knowledge.iath", help="Knowledge Base path")
    parser.add_argument("--model", type=str, help="Fine-tuned model path (optional)")

    args = parser.parse_args()
    run_inference(args.question, args.db, args.model)
"""

        with open(package_dir / "run_inference.py", "w", encoding="utf-8") as f:
            f.write(inference_script)

        os.chmod(package_dir / "run_inference.py", 0o755)

        # 5. config.json ã‚’ç”Ÿæˆ
        config = {
            "package": {
                "name": package_name,
                "generation": len(self.get_succession_history()),
                "created_at": datetime.now().isoformat(),
                "domain": domain_id,
                "version": "1.0.0"
            },
            "training_data": {
                "count": succession_result['count'],
                "min_confidence": min_confidence,
                "formats": list(succession_result['exports'].keys())
            },
            "knowledge_base": {
                "path": "knowledge_base/knowledge.iath",
                "format": "iath",
                "version": "1.0"
            },
            "usage": {
                "inference_script": "run_inference.py",
                "fine_tuning": "See README.md for instructions"
            }
        }

        with open(package_dir / "config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        # 6. ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«åœ§ç¸®
        zip_path = output_path / f"{package_name}.zip"

        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(package_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, output_path)
                    zipf.write(file_path, arcname)

        logger.info(f"Standalone package created: {zip_path}")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆZIPã®ã¿æ®‹ã™ï¼‰
        shutil.rmtree(package_dir)

        return {
            "status": "success",
            "package_path": str(zip_path),
            "package_name": package_name,
            "training_data_count": succession_result['count'],
            "generation": len(self.get_succession_history()),
            "size_bytes": os.path.getsize(zip_path)
        }


class DBArchiveManager:
    """DBä¿ç®¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, archive_dir: str = "db_archives"):
        self.archive_dir = Path(archive_dir)
        self.archive_dir.mkdir(parents=True, exist_ok=True)

    def create_snapshot(
        self,
        db_path: str,
        generation: int,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """DBã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆ"""
        import shutil

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        snapshot_name = f"generation_{generation:03d}_{timestamp}.iath"
        snapshot_path = self.archive_dir / snapshot_name

        # DBãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
        if os.path.exists(db_path):
            shutil.copy2(db_path, snapshot_path)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            metadata_path = snapshot_path.with_suffix(".json")
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump({
                    "generation": generation,
                    "timestamp": datetime.now().isoformat(),
                    "original_path": db_path,
                    "snapshot_path": str(snapshot_path),
                    "metadata": metadata or {}
                }, f, indent=2, ensure_ascii=False)

            logger.info(f"Created DB snapshot: {snapshot_path}")
            return str(snapshot_path)
        else:
            logger.error(f"DB file not found: {db_path}")
            raise FileNotFoundError(f"DB file not found: {db_path}")

    def list_snapshots(self) -> List[Dict[str, Any]]:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¸€è¦§ã‚’å–å¾—"""
        snapshots = []

        for metadata_file in sorted(self.archive_dir.glob("*.json")):
            with open(metadata_file, "r", encoding="utf-8") as f:
                snapshots.append(json.load(f))

        return snapshots

    def restore_snapshot(
        self,
        snapshot_path: str,
        target_path: str
    ) -> bool:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å¾©å…ƒ"""
        import shutil

        if not os.path.exists(snapshot_path):
            logger.error(f"Snapshot not found: {snapshot_path}")
            return False

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
        if os.path.exists(target_path):
            backup_path = f"{target_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            shutil.copy2(target_path, backup_path)
            logger.info(f"Created backup: {backup_path}")

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å¾©å…ƒ
        shutil.copy2(snapshot_path, target_path)
        logger.info(f"Restored snapshot {snapshot_path} to {target_path}")

        return True


# CLIç”¨ã®ä¾¿åˆ©é–¢æ•°
async def run_succession(
    domain_id: Optional[str] = None,
    min_confidence: float = 0.8,
    db_path: Optional[str] = None
):
    """ä¸–ä»£äº¤ä»£ã‚’å®Ÿè¡Œ"""
    history_manager = InferenceHistory()
    exporter = TrainingDataExporter()
    succession_manager = ModelSuccessionManager(history_manager, exporter)

    # ä¸–ä»£äº¤ä»£æº–å‚™
    result = await succession_manager.prepare_succession(
        domain_id=domain_id,
        min_confidence=min_confidence
    )

    print(f"\n=== Succession Result ===")
    print(f"Status: {result['status']}")
    print(f"Records: {result['count']}")
    print(f"\nExported files:")
    for format_type, path in result.get("exports", {}).items():
        print(f"  {format_type}: {path}")

    # DBã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆ
    if db_path and os.path.exists(db_path):
        archive_manager = DBArchiveManager()
        succession_history = succession_manager.get_succession_history()
        generation = len(succession_history)

        snapshot_path = archive_manager.create_snapshot(
            db_path=db_path,
            generation=generation,
            metadata={"succession_result": result}
        )
        print(f"\nDB Snapshot: {snapshot_path}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="NullAI NurseLog System")
    parser.add_argument("--domain", type=str, help="Domain ID to filter")
    parser.add_argument("--min-confidence", type=float, default=0.8, help="Minimum confidence")
    parser.add_argument("--db-path", type=str, help="Database path for snapshot")

    args = parser.parse_args()

    asyncio.run(run_succession(
        domain_id=args.domain,
        min_confidence=args.min_confidence,
        db_path=args.db_path
    ))
