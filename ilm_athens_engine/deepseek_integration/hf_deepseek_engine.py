"""
HuggingFace Transformersçµ±åˆã®DeepSeek R1æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
Ilm-Athenså…¬é–‹ç‰ˆç”¨ - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°è¿½è·¡å¯¾å¿œ
"""

import asyncio
import gc
import logging
import os
from dataclasses import dataclass, field
from typing import Dict, Optional, Any, AsyncGenerator
import torch

logger = logging.getLogger(__name__)

# HuggingFace ãƒ¢ãƒ‡ãƒ«ID
DEEPSEEK_R1_32B_MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
DEEPSEEK_R1_7B_MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"


@dataclass
class HFDeepSeekConfig:
    """HuggingFace DeepSeekè¨­å®š"""
    model_id: str = DEEPSEEK_R1_32B_MODEL_ID
    device_map: str = "auto"
    torch_dtype: str = "auto"  # "float16", "bfloat16", "auto"
    max_new_tokens: int = 2048
    temperature: float = 0.2
    top_p: float = 0.95
    do_sample: bool = True
    use_flash_attention: bool = True
    load_in_8bit: bool = False
    load_in_4bit: bool = True  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§4bité‡å­åŒ–
    trust_remote_code: bool = True
    cache_dir: Optional[str] = None
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹è¿½è·¡
    _model_loaded: bool = field(default=False, repr=False)


class HFDeepSeekEngine:
    """
    HuggingFace Transformersã‚’ä½¿ç”¨ã—ãŸDeepSeek R1æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³

    ç‰¹å¾´:
    - HuggingFace Hubã‹ã‚‰ã®ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°è¿½è·¡ï¼‰
    - 4bit/8bité‡å­åŒ–ã‚µãƒãƒ¼ãƒˆ
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ­ãƒ¼ãƒ‰/ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
    - å€’æœ¨ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ
    """

    _instance_count = 0  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ•°ã‚’è¿½è·¡

    def __init__(self, config: Optional[HFDeepSeekConfig] = None):
        self.config = config or HFDeepSeekConfig()
        self.model = None
        self.tokenizer = None
        self._is_loaded = False
        self._generation_count = 0

        HFDeepSeekEngine._instance_count += 1
        self._instance_id = HFDeepSeekEngine._instance_count

        logger.info(f"HFDeepSeekEngine ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ #{self._instance_id} ä½œæˆ")

    def load_model(self) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’HuggingFace Hubã‹ã‚‰ãƒ­ãƒ¼ãƒ‰

        Returns:
            bool: ãƒ­ãƒ¼ãƒ‰æˆåŠŸã‹ã©ã†ã‹
        """
        if self._is_loaded:
            logger.info("ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™")
            return True

        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

            logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {self.config.model_id}")
            print(f"ğŸ“¥ HuggingFaceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {self.config.model_id}")
            print("   åˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚")

            # é‡å­åŒ–è¨­å®š
            quantization_config = None
            if self.config.load_in_4bit:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            elif self.config.load_in_8bit:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)

            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_id,
                trust_remote_code=self.config.trust_remote_code,
                cache_dir=self.config.cache_dir
            )

            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            model_kwargs = {
                "trust_remote_code": self.config.trust_remote_code,
                "device_map": self.config.device_map,
                "cache_dir": self.config.cache_dir,
            }

            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
            elif self.config.torch_dtype == "float16":
                model_kwargs["torch_dtype"] = torch.float16
            elif self.config.torch_dtype == "bfloat16":
                model_kwargs["torch_dtype"] = torch.bfloat16

            # Flash Attention 2ã®ä½¿ç”¨ï¼ˆå¯¾å¿œGPUã®å ´åˆï¼‰
            if self.config.use_flash_attention:
                try:
                    model_kwargs["attn_implementation"] = "flash_attention_2"
                except Exception:
                    logger.warning("Flash Attention 2ã¯åˆ©ç”¨ä¸å¯ã€æ¨™æº–ã®attentionã‚’ä½¿ç”¨")

            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_id,
                **model_kwargs
            )

            self._is_loaded = True
            logger.info(f"âœ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {self.config.model_id}")
            print(f"âœ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°
            if torch.cuda.is_available():
                memory_gb = torch.cuda.max_memory_allocated() / 1024**3
                logger.info(f"  GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_gb:.2f} GB")
                print(f"  GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_gb:.2f} GB")

            return True

        except ImportError as e:
            logger.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: transformers, bitsandbytes ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
            print(f"   pip install transformers accelerate bitsandbytes")
            return False
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False

    def unload_model(self):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã‹ã‚‰å®Œå…¨ã«è§£æ”¾
        å€’æœ¨ã‚·ã‚¹ãƒ†ãƒ ã§ã®ä¸–ä»£äº¤ä»£æ™‚ã«ä½¿ç”¨
        """
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ (ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ #{self._instance_id})")

        if self.model is not None:
            # ãƒ¢ãƒ‡ãƒ«ã®å‚ç…§ã‚’è§£é™¤
            del self.model
            self.model = None

        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None

        self._is_loaded = False

        # GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œ
        gc.collect()

        logger.info(f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† (ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ #{self._instance_id})")

        if torch.cuda.is_available():
            memory_gb = torch.cuda.memory_allocated() / 1024**3
            logger.info(f"  è§£æ”¾å¾ŒGPU ãƒ¡ãƒ¢ãƒª: {memory_gb:.2f} GB")

    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾"""
        self.unload_model()
        HFDeepSeekEngine._instance_count -= 1

    async def infer(
        self,
        prompt: str,
        domain_context: Optional[Dict] = None,
        return_thinking: bool = True
    ) -> Dict[str, Any]:
        """
        æ¨è«–ã‚’å®Ÿè¡Œ
        """
        import time
        start_time = time.time()

        # ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã‘ã‚Œã°ãƒ­ãƒ¼ãƒ‰
        if not self._is_loaded:
            if not self.load_model():
                return {
                    "thinking": "",
                    "response": "ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "confidence": 0.0,
                    "error": "Model load failed",
                    "latency_ms": 0
                }

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        domain_name = domain_context.get("domain_name", "general") if domain_context else "general"
        optimized_prompt = self._optimize_prompt(prompt, domain_context)

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(
                optimized_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=4096
            ).to(self.model.device)

            # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¿œã˜ã¦èª¿æ•´
            gen_params = self._get_generation_params(domain_name)

            # æ¨è«–å®Ÿè¡Œï¼ˆéåŒæœŸãƒ©ãƒƒãƒ—ï¼‰
            loop = asyncio.get_event_loop()
            outputs = await loop.run_in_executor(
                None,
                lambda: self.model.generate(
                    **inputs,
                    **gen_params,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            )

            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            full_text = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = self._parse_response(full_text, return_thinking)
            result["latency_ms"] = int((time.time() - start_time) * 1000)
            result["domain"] = domain_name
            result["model_id"] = self.config.model_id

            self._generation_count += 1

            return result

        except Exception as e:
            logger.error(f"æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "thinking": "",
                "response": "",
                "confidence": 0.0,
                "error": str(e),
                "latency_ms": int((time.time() - start_time) * 1000),
                "domain": domain_name
            }

    def _get_generation_params(self, domain_name: str) -> Dict:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ã®ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—"""
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ã®æ¸©åº¦è¨­å®š
        domain_temps = {
            "medical": 0.1,
            "legal": 0.15,
            "economics": 0.3,
            "general": 0.4
        }

        temperature = domain_temps.get(domain_name, self.config.temperature)

        return {
            "max_new_tokens": self.config.max_new_tokens,
            "temperature": temperature,
            "top_p": self.config.top_p,
            "do_sample": self.config.do_sample,
            "num_return_sequences": 1,
        }

    def _optimize_prompt(self, prompt: str, domain_context: Optional[Dict] = None) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§æœ€é©åŒ–"""
        if not domain_context:
            return prompt

        domain_name = domain_context.get("domain_name", "general")
        instructions = self._get_domain_instructions(domain_name)
        return f"{instructions}\n\n{prompt}"

    def _get_domain_instructions(self, domain_name: str) -> str:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®æŒ‡ç¤ºã‚’å–å¾—"""
        from .deepseek_runner import DeepSeekR1Engine
        # æ—¢å­˜ã®å®Ÿè£…ã‚’å†åˆ©ç”¨
        temp_engine = DeepSeekR1Engine.__new__(DeepSeekR1Engine)
        return temp_engine._get_domain_instructions(domain_name)

    def _parse_response(self, full_text: str, return_thinking: bool) -> Dict:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        thinking, final_response = "", full_text

        if "<thinking>" in full_text and "</thinking>" in full_text:
            thinking_start = full_text.find("<thinking>") + len("<thinking>")
            thinking_end = full_text.find("</thinking>")
            thinking = full_text[thinking_start:thinking_end].strip()
            final_response = full_text[thinking_end + len("</thinking>"):].strip()

        confidence = self._estimate_confidence(thinking, final_response)

        return {
            "thinking": thinking if return_thinking else "",
            "response": final_response,
            "confidence": confidence,
            "full_response": full_text
        }

    def _estimate_confidence(self, thinking: str, response: str) -> float:
        """ä¿¡é ¼åº¦ã‚’æ¨å®š"""
        confidence = 0.5

        if thinking:
            word_count = len(thinking.split())
            confidence += min(0.15, word_count / 800 * 0.15)

        # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«è¨€åŠ
        evidence_keywords = ["ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«", "RCT", "ãƒ¡ã‚¿åˆ†æ", "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³"]
        for kw in evidence_keywords:
            if kw in response:
                confidence += 0.05
                break

        # æ§‹é€ åŒ–åº¦
        structure_markers = ["ã€æ¦‚è¦ã€‘", "ã€è©³ç´°èª¬æ˜ã€‘", "ã€æ³¨æ„äº‹é …ã€‘", "ã€å‚è€ƒã€‘"]
        structure_count = sum(1 for m in structure_markers if m in response)
        confidence += min(0.1, structure_count * 0.025)

        return min(0.95, max(0.1, confidence))

    @property
    def is_loaded(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹"""
        return self._is_loaded

    @property
    def generation_count(self) -> int:
        """ç”Ÿæˆå›æ•°"""
        return self._generation_count

    def get_memory_usage(self) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆGBå˜ä½ï¼‰"""
        result = {"cpu_allocated": 0.0, "gpu_allocated": 0.0, "gpu_reserved": 0.0}

        if torch.cuda.is_available():
            result["gpu_allocated"] = torch.cuda.memory_allocated() / 1024**3
            result["gpu_reserved"] = torch.cuda.memory_reserved() / 1024**3

        return result


# ã‚¨ãƒ³ã‚¸ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°
def create_engine(
    use_huggingface: bool = True,
    config: Optional[HFDeepSeekConfig] = None,
    ollama_config: Optional[Any] = None
):
    """
    é©åˆ‡ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ

    Args:
        use_huggingface: Trueãªã‚‰HuggingFaceç‰ˆã€Falseãªã‚‰Ollamaç‰ˆ
        config: HuggingFaceç”¨è¨­å®š
        ollama_config: Ollamaç”¨è¨­å®š

    Returns:
        æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    if use_huggingface:
        return HFDeepSeekEngine(config)
    else:
        from .deepseek_runner import DeepSeekR1Engine, DeepSeekConfig
        return DeepSeekR1Engine(ollama_config or DeepSeekConfig())
