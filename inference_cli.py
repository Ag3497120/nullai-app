#!/usr/bin/env python3
"""
NullAI æ¨è«–CLI - Ollamaã¨Transformersã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆ
"""
import sys
import os
import argparse
import asyncio
from typing import Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from null_ai.model_router import ModelRouter
from null_ai.config import ConfigManager


async def run_inference(
    question: str,
    provider: str = "auto",
    model: Optional[str] = None,
    domain: str = "general",
    temperature: float = 0.7,
    max_tokens: int = 2048
):
    """æ¨è«–ã‚’å®Ÿè¡Œ"""

    # è¨­å®šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    config_manager = ConfigManager()

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    if model:
        model_config = config_manager.get_model(model)
        if not model_config:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("\nåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
            for m_id, m_cfg in config_manager.models.items():
                print(f"  - {m_id}: {m_cfg.display_name} ({m_cfg.provider})")
            return
    else:
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        if provider == "ollama":
            # Ollamaãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
            model_config = next(
                (m for m in config_manager.models.values()
                 if m.provider == "ollama" and domain in m.supported_domains),
                None
            )
            if not model_config:
                print("âŒ Ollamaãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
        elif provider == "transformers":
            # Transformersãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆ
            model_config = next(
                (m for m in config_manager.models.values()
                 if m.provider == "huggingface" and domain in m.supported_domains),
                None
            )
            if not model_config:
                print("âŒ Transformersãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            model_config = config_manager.get_default_model()

    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_config.display_name}")
    print(f"ğŸ“¦ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {model_config.provider}")
    print(f"ğŸ·ï¸  ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
    print(f"â“ è³ªå•: {question}")
    print("-" * 60)

    # ModelRouteråˆæœŸåŒ–
    router = ModelRouter(
        config_manager=config_manager,
        ollama_url="http://localhost:11434"
    )

    try:
        # æ¨è«–å®Ÿè¡Œ
        result = await router.infer(
            model_id=model_config.model_id,
            prompt=question,
            temperature=temperature,
            max_tokens=max_tokens
        )

        print("\nğŸ’¬ å›ç­”:")
        print(result.get("response", "å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ"))

        if result.get("thinking"):
            print("\nğŸ§  æ€è€ƒéç¨‹:")
            print(result["thinking"])

        print(f"\nâœ… æ¨è«–å®Œäº†")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


def list_models(config_manager: ConfigManager, provider_filter: Optional[str] = None):
    """ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
    print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§:\n")

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    ollama_models = []
    transformers_models = []
    other_models = []

    for model_id, model_config in config_manager.models.items():
        if provider_filter and model_config.provider != provider_filter:
            continue

        model_info = f"  ğŸ”¹ {model_id}\n" \
                    f"      åå‰: {model_config.display_name}\n" \
                    f"      ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {model_config.provider}\n" \
                    f"      ãƒ‰ãƒ¡ã‚¤ãƒ³: {', '.join(model_config.supported_domains)}\n"

        if model_config.provider == "ollama":
            ollama_models.append(model_info)
        elif model_config.provider == "huggingface":
            transformers_models.append(model_info)
        else:
            other_models.append(model_info)

    if ollama_models:
        print("ğŸš€ Ollamaãƒ¢ãƒ‡ãƒ«:")
        print("\n".join(ollama_models))

    if transformers_models:
        print("\nğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«:")
        print("\n".join(transformers_models))

    if other_models:
        print("\nãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«:")
        print("\n".join(other_models))


def main():
    parser = argparse.ArgumentParser(
        description="NullAI æ¨è«–CLI - Ollamaã¨Transformersã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # Ollamaã§æ¨è«–
  %(prog)s --provider ollama --question "å¿ƒç­‹æ¢—å¡ã¨ã¯ï¼Ÿ" --domain medical

  # Transformersã§æ¨è«–
  %(prog)s --provider transformers --question "AIã¨ã¯ï¼Ÿ" --domain general

  # ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
  %(prog)s --model ollama-deepseek-r1-32b --question "æ³•å¾‹ã«ã¤ã„ã¦" --domain legal

  # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
  %(prog)s --list-models
  %(prog)s --list-models --provider ollama
        """
    )

    parser.add_argument(
        "-q", "--question",
        type=str,
        help="è³ªå•æ–‡"
    )

    parser.add_argument(
        "-p", "--provider",
        type=str,
        choices=["ollama", "transformers", "auto"],
        default="auto",
        help="ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ (ollama/transformers/auto)"
    )

    parser.add_argument(
        "-m", "--model",
        type=str,
        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«IDï¼ˆä¾‹: ollama-deepseek-r1-32bï¼‰"
    )

    parser.add_argument(
        "-d", "--domain",
        type=str,
        default="general",
        help="ãƒ‰ãƒ¡ã‚¤ãƒ³IDï¼ˆä¾‹: medical, legal, generalï¼‰"
    )

    parser.add_argument(
        "-t", "--temperature",
        type=float,
        default=0.7,
        help="Temperatureï¼ˆ0.0-1.0ï¼‰"
    )

    parser.add_argument(
        "--max-tokens",
        type=int,
        default=2048,
        help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
    )

    parser.add_argument(
        "--list-models",
        action="store_true",
        help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"
    )

    args = parser.parse_args()

    # è¨­å®šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    config_manager = ConfigManager()

    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    if args.list_models:
        provider_filter = None
        if args.provider != "auto":
            provider_filter = "ollama" if args.provider == "ollama" else "huggingface"
        list_models(config_manager, provider_filter)
        return

    # è³ªå•ãŒå¿…é ˆ
    if not args.question:
        parser.error("--question ãŒå¿…è¦ã§ã™ï¼ˆã¾ãŸã¯ --list-models ã‚’ä½¿ç”¨ï¼‰")

    # æ¨è«–å®Ÿè¡Œ
    asyncio.run(run_inference(
        question=args.question,
        provider=args.provider,
        model=args.model,
        domain=args.domain,
        temperature=args.temperature,
        max_tokens=args.max_tokens
    ))


if __name__ == "__main__":
    main()
